{"cells":[{"cell_type":"markdown","id":"f82bd04a","metadata":{"id":"f82bd04a"},"source":["# Interpreting Linear Regression"]},{"cell_type":"markdown","id":"245e7fe4","metadata":{"id":"245e7fe4"},"source":["In this notebook we will continue with our car price prediction example and explore the methods we use to interpret and evaluate the results of our linear regression model. We will begin by reproducing the results from the previous notebook.\n","\n","As usual, we first import the required packages and dataset."]},{"cell_type":"code","execution_count":null,"id":"a0cc2607","metadata":{"id":"a0cc2607"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plot\n","import statsmodels.api as stats\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"id":"9b6d2bc8","metadata":{"id":"9b6d2bc8"},"outputs":[],"source":["carprice_df = pd.read_csv('CarPrice_Assignment.csv')"]},{"cell_type":"markdown","id":"de9eca51","metadata":{"id":"de9eca51"},"source":["We then remove the independent variables with too many unique categories or have high correlations with other independent variables."]},{"cell_type":"code","execution_count":null,"id":"961bb3c9","metadata":{"id":"961bb3c9"},"outputs":[],"source":["carprice_df = carprice_df.drop(columns=['car_ID', 'CarName', 'carlength', 'carwidth', 'highwaympg'])"]},{"cell_type":"markdown","id":"0b9f5bbc","metadata":{"id":"0b9f5bbc"},"source":["Next we one-hot-encode the categorical variables so each category in a categorical variable becomes its own binary column - this converts categorical variables to numeric. We also remove more highly correlated variables after including the one-hot-encoded variables."]},{"cell_type":"code","execution_count":null,"id":"9648b4a7","metadata":{"id":"9648b4a7"},"outputs":[],"source":["dummy = pd.get_dummies(carprice_df.select_dtypes(include='object'), drop_first=True)"]},{"cell_type":"code","execution_count":null,"id":"a55fdeb8","metadata":{"id":"a55fdeb8"},"outputs":[],"source":["carprice_df = pd.concat([carprice_df.select_dtypes(exclude='object'), dummy], axis=1)"]},{"cell_type":"code","execution_count":null,"id":"fe6596c1","metadata":{"id":"fe6596c1"},"outputs":[],"source":["carprice_df = carprice_df.drop(columns=['compressionratio', 'drivewheel_fwd', 'enginetype_rotor', 'fuelsystem_4bbl', 'fuelsystem_idi'])"]},{"cell_type":"markdown","id":"6c9fc540","metadata":{"id":"6c9fc540"},"source":["We then split our data into train and test sets, and add our constant column."]},{"cell_type":"code","execution_count":null,"id":"be6116d3","metadata":{"id":"be6116d3"},"outputs":[],"source":["train_df=carprice_df.sample(frac=0.7, random_state=101) \n","test_df=carprice_df.drop(train_df.index)"]},{"cell_type":"code","execution_count":null,"id":"74dc7903","metadata":{"id":"74dc7903"},"outputs":[],"source":["Y_train = train_df.price\n","X_train = stats.add_constant(train_df.drop(columns=['price']))"]},{"cell_type":"markdown","id":"73f70ba4","metadata":{"id":"73f70ba4"},"source":["We then fit our model to the training data"]},{"cell_type":"code","execution_count":null,"id":"284a688d","metadata":{"id":"284a688d"},"outputs":[],"source":["model_carprice = stats.OLS(Y_train, X_train)\n","results_carprice = model_carprice.fit()"]},{"cell_type":"markdown","id":"44e8454f","metadata":{"id":"44e8454f"},"source":["We finally produce our test set predictions "]},{"cell_type":"code","execution_count":null,"id":"8f2af9ac","metadata":{"id":"8f2af9ac"},"outputs":[],"source":["Y_test = test_df.price\n","test_df = stats.add_constant(test_df)\n","X_test = test_df[X_train.columns]"]},{"cell_type":"code","execution_count":null,"id":"db55d3b3","metadata":{"id":"db55d3b3"},"outputs":[],"source":["test_predictions = results_carprice.predict(X_test)"]},{"cell_type":"markdown","id":"b14acb50","metadata":{"id":"b14acb50"},"source":["# p-Values and Coefficients"]},{"cell_type":"markdown","id":"3a49379a","metadata":{"id":"3a49379a"},"source":["### p-Values"]},{"cell_type":"markdown","id":"7a6c11fd","metadata":{"id":"7a6c11fd"},"source":["The statsmodels summary output we produced previously provides us with the p-values associated with each of our regression coefficients. These are given in the P>|t| column. Scikit-Learn does not offer any functionality to calcualte p-values so statsmodels should be used if you want to test the significance of coefficients."]},{"cell_type":"code","execution_count":null,"id":"951f97ae","metadata":{"id":"951f97ae"},"outputs":[],"source":["print(results_carprice.summary())"]},{"cell_type":"markdown","id":"3264ab5c","metadata":{"id":"3264ab5c"},"source":["We can see that many of the independent variables are not significant at the 5% level i.e. they have p-value >0.05. We can discard these variables from the model and re-train our model with only the statistically significant variables. We can repeat this until all our independent variables are significant."]},{"cell_type":"markdown","id":"285bfd5e","metadata":{"id":"285bfd5e"},"source":["### Re-training the Model"]},{"cell_type":"code","execution_count":null,"id":"b43a5baf","metadata":{"id":"b43a5baf"},"outputs":[],"source":["Y_train_new = train_df.price\n","X_train_new = stats.add_constant(train_df[['enginesize', \n","                                    'stroke', \n","                                    'peakrpm', \n","                                    'fueltype_gas', \n","                                    'carbody_hardtop', \n","                                    'carbody_hatchback', \n","                                    'enginelocation_rear',\n","                                    'enginetype_ohc',\n","                                    'cylindernumber_five',\n","                                    'cylindernumber_four',\n","                                    'cylindernumber_six']])"]},{"cell_type":"code","execution_count":null,"id":"793787e0","metadata":{"id":"793787e0"},"outputs":[],"source":["model_carprice_new = stats.OLS(Y_train_new, X_train_new)\n","results_carprice_new = model_carprice_new.fit()"]},{"cell_type":"code","execution_count":null,"id":"8e582ba5","metadata":{"id":"8e582ba5"},"outputs":[],"source":["print(results_carprice_new.summary())"]},{"cell_type":"markdown","id":"7b2bec40","metadata":{"id":"7b2bec40"},"source":["All of our independent variables now have p-values below 0.05 (except const which we are not concerned whether this is zero or not) and we have reduced the complexity of our model by reducing the number of variables."]},{"cell_type":"code","execution_count":null,"id":"b6fdba80","metadata":{"id":"b6fdba80"},"outputs":[],"source":["Y_test_new = test_df.price\n","X_test_new = test_df[X_train_new.columns]"]},{"cell_type":"code","execution_count":null,"id":"ddc35709","metadata":{"id":"ddc35709"},"outputs":[],"source":["test_predictions_new = results_carprice_new.predict(X_test_new)"]},{"cell_type":"code","execution_count":null,"id":"ffef9518","metadata":{"id":"ffef9518"},"outputs":[],"source":["plot.scatter(test_predictions_new, Y_test_new)\n","plot.plot([5000, 50000], [5000, 50000], c='k', ls='--')\n","plot.xlabel('Predicted Price [$]')\n","plot.ylabel('Observed Price [$]')\n","plot.show()"]},{"cell_type":"markdown","id":"056f7bc6","metadata":{"id":"056f7bc6"},"source":["### Coefficients"]},{"cell_type":"markdown","id":"91ab5758","metadata":{"id":"91ab5758"},"source":["The coefficients tell us how much the target variable changes for a one unit change in the corresponding independent variable when all the other independent variables are held fixed. We can get a measure of which independent variable contributes the most change to the target variable if we first scale all the independent variables to remove the units. This is known as standardization as we discussed in the theory lesson."]},{"cell_type":"code","execution_count":null,"id":"c9c84bfd","metadata":{"id":"c9c84bfd"},"outputs":[],"source":["X_train_scale = ((X_train_new - X_train_new.mean())/X_train_new.std()).drop(columns=['const'])"]},{"cell_type":"code","execution_count":null,"id":"bf44aa89","metadata":{"id":"bf44aa89"},"outputs":[],"source":["X_train_scale.std()"]},{"cell_type":"code","execution_count":null,"id":"a382976b","metadata":{"id":"a382976b"},"outputs":[],"source":["X_train_scale.mean().round(2)"]},{"cell_type":"markdown","id":"97e4b433","metadata":{"id":"97e4b433"},"source":["As we can see the independent variables now all have mean zero and a standard deviation of one and will will therefore be able to compare coefficient values. We will now refit the model. Scikit-Learn also contains a function [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) which will perform this standardization."]},{"cell_type":"code","execution_count":null,"id":"e0062b6a","metadata":{"id":"e0062b6a"},"outputs":[],"source":["X_train_scale = stats.add_constant(X_train_scale)"]},{"cell_type":"markdown","id":"2a5deb02","metadata":{"id":"2a5deb02"},"source":["We add our constant back in as we removed this in the above step since standardization doesn't work for a constant value. This is because both the numerator and denominator are zero and dividing zero by zero doesn't make sense to us. We then fit our model to this standardized data. "]},{"cell_type":"code","execution_count":null,"id":"76832e92","metadata":{"id":"76832e92"},"outputs":[],"source":["model_carprice_scale = stats.OLS(Y_train_new, X_train_scale)\n","results_carprice_scale = model_carprice_scale.fit()"]},{"cell_type":"code","execution_count":null,"id":"8d8a2064","metadata":{"id":"8d8a2064"},"outputs":[],"source":["print(results_carprice_scale.summary())"]},{"cell_type":"markdown","id":"ff8740df","metadata":{"id":"ff8740df"},"source":["The magnitudes of the coefficients now tell us how much they contribute to the change in price relative to each other. We can plot these in a bar chart using the [df.plot.bar()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.bar.html) method. We use the .abs() method to make all the values positive since we are only concerned with the magnitudes."]},{"cell_type":"code","execution_count":null,"id":"87ba2fea","metadata":{"id":"87ba2fea"},"outputs":[],"source":["results_carprice_scale.params.drop(index=['const']).abs().sort_values(ascending=False).plot.bar()"]},{"cell_type":"markdown","id":"b6c0a9f1","metadata":{"id":"b6c0a9f1"},"source":["Here we can see that a change in engine size contributes the biggest change to the price while whether the car is a hatchback or not contributes the least. In a sense this gives us how important each feature is relatively to the model."]},{"cell_type":"markdown","id":"5178a0e4","metadata":{"id":"5178a0e4"},"source":["# Residuals and Residual Plots"]},{"cell_type":"markdown","id":"5900fe95","metadata":{"id":"5900fe95"},"source":["We can assess our model using the residuals between the predicted values and the observed values. statsmodels results instances have a method [.resid](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html) for calculating these residuals. If we are using Scikit-Learn then we can manually calculate the residuals by subtracting the predicted values from the observed values."]},{"cell_type":"code","execution_count":null,"id":"ceae9b81","metadata":{"id":"ceae9b81"},"outputs":[],"source":["results_carprice_new.resid"]},{"cell_type":"markdown","id":"253066c1","metadata":{"id":"253066c1"},"source":["We can then plot these residuals against our predicted values for the training data and observe if we see any non-random pattern in the data. "]},{"cell_type":"code","execution_count":null,"id":"78f00917","metadata":{"id":"78f00917"},"outputs":[],"source":["plot.scatter(results_carprice_new.fittedvalues, results_carprice_new.resid)\n","plot.plot([5000,50000], [0,0], c='k', ls='--')"]},{"cell_type":"markdown","id":"3b7e1d63","metadata":{"id":"3b7e1d63"},"source":["As we can see in the plot above, there seems to be no discernible pattern in the residuals which is promising.\n","#### Durbin-Watson Test\n","The results summary above also provides us with a [Durbin-Watson](https://www.investopedia.com/terms/d/durbin-watson-statistic.asp) metric. This tests the residuals for any possible autocorrelation and will have a value between 0 and 4. A value of 2 means no autocorrelation and values between 1.5 and 2.5 are relatively normal when the data is not especially autocorrelated. The summary gives us a value of 1.85 which is not a cause for concern. \n","\n","#### Breusch-Pagan test\n","We can also do a quick test for heteroskedasticity using the [Breusch-Pagan](https://www.statology.org/breusch-pagan-test/) test. Here we calculate the value $nR^{2}$ where $n$ is the number of datapoints in the model (in our case 144) and $R^{2}$ is the coefficient of determination. This is given to us in the results summary where it is shown as R-squared (our value is 0.883). This value is distributed as a $\\chi^{2}$ variable with degrees of freedom of $n-p-1$ where $p$ is our number of independent variables in the model (the -1 is for the intercept value). From this we can get a p-value where our null hypothesis is that the data is homoscedastic (constant variance) i.e. the probabiltity that our data is homoscedastic. If our p-value is less that 0.05 then it is significant and our data is likely heteroskedastic.\n","From our data we get a value for $nR^{2}$ of 127 and we have $144-11-1=132$ degrees of freedom. We can use these two values to [calculate](https://www.statology.org/chi-square-p-value-calculator/) our p-value. For our data we get a p-value of 0.6 and as such this is not significant and we can treat the data as homoscedastic. "]},{"cell_type":"markdown","id":"b5176b4e","metadata":{"id":"b5176b4e"},"source":["As we are now satisfied that a linear regression model is appropriate and our data doesn't violate any assumptions required to use ordinary least squares, we can move on to evaluating how good our model is with various metrics. "]},{"cell_type":"markdown","id":"7648fa51","metadata":{"id":"7648fa51"},"source":["# Evaluating Linear Regression"]},{"cell_type":"markdown","id":"5dad1217","metadata":{"id":"5dad1217"},"source":["statsmodels allows us to easily calculate the metrics we have discussed ([mean square error](https://www.statsmodels.org/stable/generated/statsmodels.tools.eval_measures.mse.html), [root mean square error](https://www.statsmodels.org/stable/generated/statsmodels.tools.eval_measures.rmse.html), [mean absolute error](https://www.statsmodels.org/stable/generated/statsmodels.tools.eval_measures.meanabs.html), [$R^{2}$](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.rsquared.html) and [adjusted $R^{2}$](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLSResults.rsquared_adj.html)) We can calculate these for either/both the training data and the test data."]},{"cell_type":"code","execution_count":null,"id":"34bdb81d","metadata":{"scrolled":true,"id":"34bdb81d"},"outputs":[],"source":["train_mse = stats.tools.eval_measures.mse(Y_train_new, results_carprice_new.fittedvalues)\n","print('The training dataset mean square error is {}'.format(train_mse.round(1)))"]},{"cell_type":"code","execution_count":null,"id":"694451c8","metadata":{"id":"694451c8"},"outputs":[],"source":["train_rmse = stats.tools.eval_measures.rmse(Y_train_new, results_carprice_new.fittedvalues)\n","print('The training dataset root mean square error is {}'.format(train_rmse.round(1)))"]},{"cell_type":"code","execution_count":null,"id":"7718a85e","metadata":{"id":"7718a85e"},"outputs":[],"source":["train_mae = stats.tools.eval_measures.meanabs(Y_train_new, results_carprice_new.fittedvalues)\n","print('The training dataset mean absolute error is {}'.format(train_mae.round(1)))"]},{"cell_type":"code","execution_count":null,"id":"91ae735f","metadata":{"id":"91ae735f"},"outputs":[],"source":["train_r2 = results_carprice_new.rsquared\n","print('The training dataset coefficient of determination is {}'.format(train_r2.round(3)))"]},{"cell_type":"code","execution_count":null,"id":"eaf56881","metadata":{"scrolled":true,"id":"eaf56881"},"outputs":[],"source":["train_r2_adj = results_carprice_new.rsquared_adj\n","print('The training dataset adjusted coefficient of determination is {}'.format(train_r2_adj.round(3)))"]},{"cell_type":"markdown","id":"7bf79b69","metadata":{"id":"7bf79b69"},"source":["We can see above in the root mean square error that our predictions are typically around \\\\$3000 off from the observed values and that the data explains 88.3% of the variation from the coefficient of determination. \n","\n","The coefficient of determination and root mean square error for our simple linear regression model from Chapter 2 were 0.75 and \\\\$4000 respectively so we are explaining more of the variation and getting better (on average) predictions with our multiple independent variables. As the adjusted coefficient of determination is similar here we can be reasonably confident that the addition of more independent variables is not simply adding random fluctuations to the model."]},{"cell_type":"markdown","id":"a0f8b771","metadata":{"id":"a0f8b771"},"source":["### Scikit-Learn"]},{"cell_type":"markdown","id":"c83f592e","metadata":{"id":"c83f592e"},"source":["Again we can repeat using the Scikit-Learn package and calculate our metrics. We will use the same independent variables we used above after calculating the p-values."]},{"cell_type":"code","execution_count":null,"id":"49c3fbd2","metadata":{"id":"49c3fbd2"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"id":"e9161357","metadata":{"id":"e9161357"},"outputs":[],"source":["Y = carprice_df.price\n","X = carprice_df[['enginesize', \n","                 'stroke', \n","                 'peakrpm', \n","                 'fueltype_gas', \n","                 'carbody_hardtop', \n","                 'carbody_hatchback', \n","                 'enginelocation_rear',\n","                 'enginetype_ohc',\n","                 'cylindernumber_five',\n","                 'cylindernumber_four',\n","                 'cylindernumber_six']]"]},{"cell_type":"markdown","id":"b5d50f0d","metadata":{"id":"b5d50f0d"},"source":["As before, we split our data into test and train sets and fit our model on the training data."]},{"cell_type":"code","execution_count":null,"id":"42830e76","metadata":{"id":"42830e76"},"outputs":[],"source":["sk_X_train, sk_X_test, sk_Y_train, sk_Y_test = train_test_split(X, Y, test_size=0.3, random_state=97)"]},{"cell_type":"code","execution_count":null,"id":"141cf3ea","metadata":{"id":"141cf3ea"},"outputs":[],"source":["regressor = LinearRegression()  \n","regressor.fit(sk_X_train, sk_Y_train)"]},{"cell_type":"code","execution_count":null,"id":"9ec1186d","metadata":{"id":"9ec1186d"},"outputs":[],"source":["sk_intercept_carprice = regressor.intercept_\n","sk_engsize_coeffs = regressor.coef_\n","sk_ssr_carprice = np.sum((sk_Y_train-regressor.predict(sk_X_train))**2)"]},{"cell_type":"markdown","id":"1f217db5","metadata":{"id":"1f217db5"},"source":["Here we use our model to make our predictions for both the training and test data. "]},{"cell_type":"code","execution_count":null,"id":"dd44fa68","metadata":{"id":"dd44fa68"},"outputs":[],"source":["sk_train_predictions = regressor.predict(sk_X_train)\n","sk_test_predictions = regressor.predict(sk_X_test)"]},{"cell_type":"markdown","id":"cd0eeed0","metadata":{"id":"cd0eeed0"},"source":["We can extract the mean squared error, mean absolute error and $R^{2}$ using Scikit-Learn function from the module sklearn.metrics. Adjusted $R^{2}$ needs to be calculated manually from $R^{2}$ and the root mean square error can be taken as the square root of the MSE. These functions take our observed target values and predicted values as arguments."]},{"cell_type":"code","execution_count":null,"id":"66e0eafe","metadata":{"id":"66e0eafe"},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"]},{"cell_type":"code","execution_count":null,"id":"d68b2213","metadata":{"id":"d68b2213"},"outputs":[],"source":["sk_train_mse = mean_squared_error(sk_Y_train, sk_train_predictions)\n","print('The training dataset mean square error is {}'.format(sk_train_mse.round(1)))"]},{"cell_type":"code","execution_count":null,"id":"5c87d194","metadata":{"id":"5c87d194"},"outputs":[],"source":["sk_train_rmse = np.sqrt(sk_train_mse)\n","print('The training dataset root mean square error is {}'.format(sk_train_rmse.round(1)))"]},{"cell_type":"code","execution_count":null,"id":"9d90bdeb","metadata":{"id":"9d90bdeb"},"outputs":[],"source":["sk_train_mae = mean_absolute_error(sk_Y_train, sk_train_predictions)\n","print('The training dataset mean absolute error is {}'.format(sk_train_mae.round(1)))"]},{"cell_type":"code","execution_count":null,"id":"e96ddd58","metadata":{"id":"e96ddd58"},"outputs":[],"source":["sk_train_r2 = r2_score(sk_Y_train, sk_train_predictions)\n","print('The training dataset coefficient of determination is {}'.format(sk_train_r2.round(3)))"]},{"cell_type":"markdown","id":"b601ced3","metadata":{"id":"b601ced3"},"source":["we calculate the adjusted $R^{2}$ using the following formula (where we have substituted the equation for $R^{2}$ into our equation for the adjusted $R^{2}$ shown in the theory lesson).\n","\n","$ 1-\\frac{(1-R^2)(n-1)}{(n-p-1)}$\n","\n","- n is the number of observations in our data\n","- p is the number of independent variables we have"]},{"cell_type":"code","execution_count":null,"id":"0fe536ea","metadata":{"id":"0fe536ea"},"outputs":[],"source":["n = sk_X_train.shape[0]\n","p = sk_X_train.shape[1]\n","sk_train_r2_adj = 1-(1-sk_train_r2)*(n-1)/(n-p-1)\n","print('The training dataset adjusted coefficient of determination is {}'.format(sk_train_r2_adj.round(3)))"]},{"cell_type":"markdown","id":"9dcaf7ff","metadata":{"id":"9dcaf7ff"},"source":["### Test Data"]},{"cell_type":"markdown","id":"dd081baf","metadata":{"id":"dd081baf"},"source":["Scikit-learn also easily allows us to evaluate these metrics on our test data to see how well our model performs on unseen data. This is key when we want to use our model to predict new values. Below we calculate our $R^{2}$ value from the test data."]},{"cell_type":"code","execution_count":null,"id":"0bb39f30","metadata":{"id":"0bb39f30"},"outputs":[],"source":["sk_test_r2 = r2_score(sk_Y_test, sk_test_predictions)\n","print('The test dataset coefficient of determination is {}'.format(sk_test_r2.round(3)))"]},{"cell_type":"markdown","id":"3dedffe2","metadata":{"id":"3dedffe2"},"source":["As we can see, our model works well on the test data, explaining 84% of the variance. It is typical that even in a good model the score on the test data will be slightly lower than on the training data as the model was built on the training data. If the test score is significantly lower than the train score then this is an indicator that our model is overfitting the training data as the model is too specific to the data it is trained on."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"Evaluating Linear Regression - Complete.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}